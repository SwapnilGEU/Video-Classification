{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a3d6c74",
   "metadata": {},
   "source": [
    "# DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18cbf2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (2.7.0+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (2.7.0+cu126)\n",
      "Requirement already satisfied: filelock in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\swapn\\downloads\\intern\\projects\\c3d\\env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6c6ae",
   "metadata": {},
   "source": [
    "Counting the no of files in each folder for picking labels with large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c496e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted file counts in 'train' directory by class (ascending):\n",
      "  BreastStroke: 75 files\n",
      "  PlayingViolin: 75 files\n",
      "  PullUps: 75 files\n",
      "  Skijet: 75 files\n",
      "  TaiChi: 75 files\n",
      "  PushUps: 76 files\n",
      "  PlayingPiano: 78 files\n",
      "  UnevenBars: 78 files\n",
      "  BalanceBeam: 81 files\n",
      "  BlowingCandles: 81 files\n",
      "  CuttingInKitchen: 82 files\n",
      "  MoppingFloor: 82 files\n",
      "  SkyDiving: 82 files\n",
      "  Fencing: 83 files\n",
      "  HandstandWalking: 83 files\n",
      "  PlayingTabla: 83 files\n",
      "  Rafting: 83 files\n",
      "  BodyWeightSquats: 84 files\n",
      "  CleanAndJerk: 84 files\n",
      "  PizzaTossing: 84 files\n",
      "  StillRings: 84 files\n",
      "  ApplyLipstick: 85 files\n",
      "  ParallelBars: 85 files\n",
      "  JavelinThrow: 87 files\n",
      "  SumoWrestling: 87 files\n",
      "  VolleyballSpiking: 87 files\n",
      "  RopeClimbing: 89 files\n",
      "  TrampolineJumping: 89 files\n",
      "  JugglingBalls: 90 files\n",
      "  SkateBoarding: 90 files\n",
      "  HighJump: 92 files\n",
      "  JumpingJack: 92 files\n",
      "  Knitting: 92 files\n",
      "  PommelHorse: 92 files\n",
      "  WalkingWithDog: 92 files\n",
      "  FloorGymnastics: 93 files\n",
      "  HorseRace: 93 files\n",
      "  HulaHoop: 93 files\n",
      "  MilitaryParade: 93 files\n",
      "  FieldHockeyPenalty: 94 files\n",
      "  FrisbeeCatch: 94 files\n",
      "  Surfing: 94 files\n",
      "  Lunges: 95 files\n",
      "  HandstandPushups: 96 files\n",
      "  YoYo: 96 files\n",
      "  Haircut: 97 files\n",
      "  ThrowDiscus: 97 files\n",
      "  WallPushups: 97 files\n",
      "  BasketballDunk: 98 files\n",
      "  BlowDryHair: 98 files\n",
      "  BrushingTeeth: 98 files\n",
      "  LongJump: 98 files\n",
      "  Swing: 98 files\n",
      "  BabyCrawling: 99 files\n",
      "  Nunchucks: 99 files\n",
      "  SalsaSpin: 99 files\n",
      "  Biking: 100 files\n",
      "  BoxingSpeedBag: 100 files\n",
      "  Skiing: 101 files\n",
      "  FrontCrawl: 102 files\n",
      "  Mixing: 102 files\n",
      "  Rowing: 102 files\n",
      "  SoccerPenalty: 102 files\n",
      "  Typing: 102 files\n",
      "  CliffDiving: 103 files\n",
      "  CricketBowling: 104 files\n",
      "  GolfSwing: 104 files\n",
      "  Hammering: 105 files\n",
      "  Kayaking: 105 files\n",
      "  TableTennisShot: 105 files\n",
      "  ApplyEyeMakeup: 108 files\n",
      "  Archery: 108 files\n",
      "  JumpRope: 108 files\n",
      "  RockClimbingIndoor: 108 files\n",
      "  Shotput: 108 files\n",
      "  HeadMassage: 110 files\n",
      "  SoccerJuggling: 110 files\n",
      "  PoleVault: 111 files\n",
      "  BaseballPitch: 112 files\n",
      "  Billiards: 112 files\n",
      "  Diving: 112 files\n",
      "  HammerThrow: 112 files\n",
      "  PlayingDaf: 113 files\n",
      "  WritingOnBoard: 114 files\n",
      "  BandMarching: 116 files\n",
      "  Bowling: 116 files\n",
      "  PlayingFlute: 116 files\n",
      "  PlayingSitar: 117 files\n",
      "  IceDancing: 118 files\n",
      "  BenchPress: 120 files\n",
      "  Drumming: 120 files\n",
      "  PlayingGuitar: 120 files\n",
      "  Punch: 120 files\n",
      "  ShavingBeard: 120 files\n",
      "  BoxingPunchingBag: 122 files\n",
      "  HorseRiding: 123 files\n",
      "  PlayingCello: 123 files\n",
      "  PlayingDhol: 123 files\n",
      "  TennisSwing: 124 files\n",
      "  CricketShot: 125 files\n",
      "  Basketball: 198 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to count files in each class directory\n",
    "def count_files_per_class(parent_directory):\n",
    "    class_counts = {}\n",
    "    # Iterate through each subdirectory (class)\n",
    "    for class_name in os.listdir(parent_directory):\n",
    "        class_path = os.path.join(parent_directory, class_name)\n",
    "        if os.path.isdir(class_path):  # Ensure it's a directory\n",
    "            # Count files in this class directory\n",
    "            num_files = len([f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))])\n",
    "            class_counts[class_name] = num_files\n",
    "    return class_counts\n",
    "\n",
    "# Function to sort the classes by file count\n",
    "def sort_class_counts(class_counts):\n",
    "    return sorted(class_counts.items(), key=lambda x: x[1])  # Sort by the count (value)\n",
    "\n",
    "train_dir = r\"data\\train\"  \n",
    "\n",
    "\n",
    "\n",
    "train_class_counts = count_files_per_class(train_dir)\n",
    "\n",
    "\n",
    "sorted_train_counts = sort_class_counts(train_class_counts)\n",
    "\n",
    "# Display sorted counts for each directory\n",
    "print(\"Sorted file counts in 'train' directory by class (ascending):\")\n",
    "for class_name, count in sorted_train_counts:\n",
    "    print(f\"  {class_name}: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311c917",
   "metadata": {},
   "source": [
    "Extracting frames and storing it in Tensor and Numpy format for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92095d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class UCF101Dataset(Dataset):\n",
    "    def __init__(self, csv_path, clip_len=16, transform=None):\n",
    "\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.clip_len = clip_len\n",
    "        self.transform = transform\n",
    "        selected_labels = ['PlayingCello','PlayingDhol','TennisSwing','CricketShot','HorseRiding']\n",
    "        self.data = self.data[self.data['label'].isin(selected_labels)]\n",
    "\n",
    "        '''Only include 5 classes\n",
    "        selected_labels = sorted(self.data['label'].unique())[:5]  # or specify manually\n",
    "        self.data = self.data[self.data['label'].isin(selected_labels)]'''\n",
    "\n",
    "        '''Ensure the dataset is balanced by sampling 5 clips per label\n",
    "        self.data = self.data.groupby('label', group_keys=False).apply(lambda x: x.sample(n=5, random_state=42)).reset_index(drop=True)'''\n",
    "\n",
    "       \n",
    "        \n",
    "        # Build label-to-index mapping from unique labels\n",
    "        self.label2index = {label: idx for idx, label in enumerate(sorted(self.data['label'].unique()))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        root_dir = r\"C:\\Users\\Swapn\\Downloads\\Intern\\Projects\\c3d\\data\"\n",
    "        video_rel_path = self.data.iloc[idx]['clip_path']\n",
    "        video_path = os.path.join(root_dir, video_rel_path)\n",
    "\n",
    "        label_str = self.data.iloc[idx]['label']\n",
    "        label_idx = self.label2index[label_str]  # Convert label string to integer\n",
    "\n",
    "        frames = self.load_video(video_path)\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        return frames, label_idx  # Return label as integer\n",
    "\n",
    "    def load_video(self, path):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (112, 112))\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        #Video format is not Readable\n",
    "        if len(frames) == 0:\n",
    "            frames = [np.zeros((112, 112, 3), dtype=np.uint8).copy() for _ in range(self.clip_len)]\n",
    "        #If the clips to shoty copy the previous frame\n",
    "        elif len(frames) < self.clip_len:\n",
    "            last_frame = frames[-1]\n",
    "            frames.extend([last_frame.copy() for _ in range(self.clip_len - len(frames))])\n",
    "        # If the video is too long truncate \n",
    "        else:\n",
    "            frames = frames[:self.clip_len] \n",
    "\n",
    "        frames = np.array(frames).astype(np.float32) / 255.0  \n",
    "        frames = frames.transpose(3, 0, 1, 2)  \n",
    "\n",
    "        return torch.from_numpy(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da032dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of train 1\n",
      "Frames shape: torch.Size([5, 3, 16, 112, 112])\n",
      "Label: tensor([4, 1, 4, 0, 0])\n",
      "First frame shape: torch.Size([3, 112, 112])\n",
      "Batch of val 1\n",
      "Frames shape: torch.Size([5, 3, 16, 112, 112])\n",
      "Label: tensor([3, 3, 4, 0, 3])\n",
      "First frame shape: torch.Size([3, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_path1 = r\"C:\\Users\\Swapn\\Downloads\\Intern\\Projects\\c3d\\data\\train.csv\"\n",
    "\n",
    "csv_path2 = r\"C:\\Users\\Swapn\\Downloads\\Intern\\Projects\\c3d\\data\\val.csv\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # Add other transformations if needed (like normalization)\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset1 = UCF101Dataset(csv_path=csv_path1, clip_len=16, transform=transform)\n",
    "dataset2 = UCF101Dataset(csv_path=csv_path2, clip_len=16, transform=transform)\n",
    "\n",
    "# Create a DataLoader (optional, for batching)\n",
    "train_loader = DataLoader(dataset1, batch_size=5, shuffle=True)\n",
    "val_loader = DataLoader(dataset2, batch_size=5, shuffle=True)\n",
    "\n",
    "for i, (frames, label) in enumerate(train_loader):\n",
    "    print(f\"Batch of train {i+1}\")\n",
    "    print(f\"Frames shape: {frames.shape}\")  \n",
    "    print(f\"Label: {label}\")\n",
    "\n",
    "\n",
    "    print(f\"First frame shape: {frames[0, :, 0, :, :].shape}\")  \n",
    "    if i == 0:  \n",
    "        break\n",
    "for i, (frames, label) in enumerate(val_loader):\n",
    "    print(f\"Batch of val {i+1}\")\n",
    "    print(f\"Frames shape: {frames.shape}\")  \n",
    "    print(f\"Label: {label}\")\n",
    "\n",
    "\n",
    "    print(f\"First frame shape: {frames[0, :, 0, :, :].shape}\")  \n",
    "    if i == 0:  \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cce0c78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize your dataset\n",
    "dataset = UCF101Dataset(\n",
    "    csv_path=r'C:\\Users\\Swapn\\Downloads\\Intern\\Projects\\c3d\\data\\train.csv',  # ðŸ‘ˆ put your actual CSV file path here\n",
    "    clip_len=16,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Get one clip from the dataset\n",
    "frames, label = dataset[i]  \n",
    "\n",
    "# Convert frames from [C, T, H, W] â†’ [T, H, W, C] for plotting\n",
    "frames_np = frames.permute(1, 2, 3, 0).numpy()  # shape: [T, H, W, C]\n",
    "\n",
    "# Plot the first few frames\n",
    "num_frames_to_show = min(8, frames_np.shape[0])\n",
    "plt.figure(figsize=(16, 4))\n",
    "for i in range(num_frames_to_show):\n",
    "    plt.subplot(1, num_frames_to_show, i + 1)\n",
    "    plt.imshow(frames_np[i])\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"F{i+1}\")\n",
    "\n",
    "plt.suptitle(f\"Label Index: {label}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2200d8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nimport torch.nn as nn\\n\\nclass C3DHybridSVM(nn.Module):\\n    def __init__(self, num_classes):\\n        super(C3DHybridSVM, self).__init__()\\n\\n        # C3D convolutional layers\\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\\n        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\\n\\n        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\\n        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\\n\\n        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\\n        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\\n        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\\n\\n        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\\n        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\\n        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\\n\\n        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\\n        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\\n        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\\n\\n        # Fully connected layers for feature extraction\\n        self.fc6 = nn.Linear(8192, 4096)\\n        self.fc7 = nn.Linear(4096, 4096)\\n\\n        # Linear SVM layer (replacing fc8)\\n        self.svm_linear = nn.Linear(4096, num_classes)\\n\\n        # Other components\\n        self.dropout = nn.Dropout(p=0.5)\\n        self.relu = nn.ReLU()\\n\\n    def forward(self, x):\\n        # C3D feature extraction\\n        h = self.relu(self.conv1(x))\\n        h = self.pool1(h)\\n\\n        h = self.relu(self.conv2(h))\\n        h = self.pool2(h)\\n\\n        h = self.relu(self.conv3a(h))\\n        h = self.relu(self.conv3b(h))\\n        h = self.pool3(h)\\n\\n        h = self.relu(self.conv4a(h))\\n        h = self.relu(self.conv4b(h))\\n        h = self.pool4(h)\\n\\n        h = self.relu(self.conv5a(h))\\n        h = self.relu(self.conv5b(h))\\n        h = self.pool5(h)\\n\\n        h = h.view(-1, 8192)\\n        h = self.relu(self.fc6(h))\\n        h = self.dropout(h)\\n        h = self.relu(self.fc7(h))\\n        h = self.dropout(h)\\n\\n        # SVM linear layer (outputs raw scores)\\n        scores = self.svm_linear(h)\\n\\n        return scores\\n\\n    def get_features(self, x):\\n        \"\"\"Extract features from the fc7 layer for external SVM training.\"\"\"\\n        h = self.relu(self.conv1(x))\\n        h = self.pool1(h)\\n\\n        h = self.relu(self.conv2(h))\\n        h = self.pool2(h)\\n\\n        h = self.relu(self.conv3a(h))\\n        h = self.relu(self.conv3b(h))\\n        h = self.pool3(h)\\n\\n        h = self.relu(self.conv4a(h))\\n        h = self.relu(self.conv4b(h))\\n        h = self.pool4(h)\\n\\n        h = self.relu(self.conv5a(h))\\n        h = self.relu(self.conv5b(h))\\n        h = self.pool5(h)\\n\\n        h = h.view(-1, 8192)\\n        h = self.relu(self.fc6(h))\\n        h = self.dropout(h)\\n        h = self.relu(self.fc7(h))\\n        return h'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class C3DHybridSVM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(C3DHybridSVM, self).__init__()\n",
    "\n",
    "        # C3D convolutional layers\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n",
    "\n",
    "        # Fully connected layers for feature extraction\n",
    "        self.fc6 = nn.Linear(8192, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "\n",
    "        # Linear SVM layer (replacing fc8)\n",
    "        self.svm_linear = nn.Linear(4096, num_classes)\n",
    "\n",
    "        # Other components\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # C3D feature extraction\n",
    "        h = self.relu(self.conv1(x))\n",
    "        h = self.pool1(h)\n",
    "\n",
    "        h = self.relu(self.conv2(h))\n",
    "        h = self.pool2(h)\n",
    "\n",
    "        h = self.relu(self.conv3a(h))\n",
    "        h = self.relu(self.conv3b(h))\n",
    "        h = self.pool3(h)\n",
    "\n",
    "        h = self.relu(self.conv4a(h))\n",
    "        h = self.relu(self.conv4b(h))\n",
    "        h = self.pool4(h)\n",
    "\n",
    "        h = self.relu(self.conv5a(h))\n",
    "        h = self.relu(self.conv5b(h))\n",
    "        h = self.pool5(h)\n",
    "\n",
    "        h = h.view(-1, 8192)\n",
    "        h = self.relu(self.fc6(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(self.fc7(h))\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        # SVM linear layer (outputs raw scores)\n",
    "        scores = self.svm_linear(h)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def get_features(self, x):\n",
    "        \"\"\"Extract features from the fc7 layer for external SVM training.\"\"\"\n",
    "        h = self.relu(self.conv1(x))\n",
    "        h = self.pool1(h)\n",
    "\n",
    "        h = self.relu(self.conv2(h))\n",
    "        h = self.pool2(h)\n",
    "\n",
    "        h = self.relu(self.conv3a(h))\n",
    "        h = self.relu(self.conv3b(h))\n",
    "        h = self.pool3(h)\n",
    "\n",
    "        h = self.relu(self.conv4a(h))\n",
    "        h = self.relu(self.conv4b(h))\n",
    "        h = self.pool4(h)\n",
    "\n",
    "        h = self.relu(self.conv5a(h))\n",
    "        h = self.relu(self.conv5b(h))\n",
    "        h = self.pool5(h)\n",
    "\n",
    "        h = h.view(-1, 8192)\n",
    "        h = self.relu(self.fc6(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(self.fc7(h))\n",
    "        return h'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f4d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model without SVM and Validation function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class C3D(nn.Module):\n",
    "    def __init__(self, num_classes=101):\n",
    "        super(C3D, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
    "\n",
    "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "\n",
    "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "\n",
    "            nn.Conv3d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
    "\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        )\n",
    "\n",
    "        # Dynamically determine the flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 16, 112, 112)\n",
    "            dummy_output = self.features(dummy_input)\n",
    "            self.flattened_size = dummy_output.view(1, -1).size(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80da97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([2, 2, 3, 3, 1])\n",
      "<class 'torch.Tensor'> tensor([2, 2, 3, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "if 'labels' not in locals():\n",
    "\tfor i, (frames, labels) in enumerate(train_loader):\n",
    "\t\tbreak  \n",
    "\n",
    "print(type(labels), labels)\n",
    "\n",
    "if 'labels' not in locals():\n",
    "\tfor i, (frames, labels) in enumerate(val_loader):\n",
    "\t\tbreak  \n",
    "\n",
    "print(type(labels), labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76fa1307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to model: torch.Size([5, 3, 16, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'frames' not in locals():\n",
    "\tfor i, (frames, label) in enumerate(train_loader):\n",
    "\t\tbreak  \n",
    "\n",
    "print(\"Input to model:\", frames.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725dba40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f29a1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(outputs, targets, margin=1.0):\n",
    "    \"\"\"Multi-class hinge loss (SVM-style).\"\"\"\n",
    "    batch_size = outputs.size(0)\n",
    "    correct_class_scores = outputs[range(batch_size), targets].view(-1, 1)\n",
    "    margins = torch.clamp(outputs - correct_class_scores + margin, min=0)\n",
    "    margins[range(batch_size), targets] = 0  # zero out the correct class\n",
    "    return margins.sum() / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915f32b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cfa0cb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C3DHybridSVM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Instantiate model and optimizer\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mC3DHybridSVM\u001b[49m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Training parameters\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'C3DHybridSVM' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define device - this line was incomplete and duplicated\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate model and optimizer\n",
    "model = C3DHybridSVM(num_classes=5).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "\n",
    "# Ensure data_loader is defined\n",
    "if 'train_loader' not in locals():\n",
    "    raise NameError(\"train_loader is not defined. Please execute the cell defining data_loader before running this cell.\")\n",
    "\n",
    "if 'val_loader' not in locals():\n",
    "    raise NameError(\"val_loader is not defined. Please execute the cell defining data_loader before running this cell.\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    loop1 = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "    \n",
    "    for frames, labels in loop1:\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.to(device, dtype=torch.long)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(frames)\n",
    "        loss = hinge_loss(outputs, labels, margin=1.0)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Stats\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        loop1.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/len(train_loader):.4f} Accuracy: {100 * correct / total:.2f}%\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    loop2 = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, labels in loop2:\n",
    "            frames, labels = frames.to(device), labels.to(device, dtype=torch.long)\n",
    "            outputs = model(frames)\n",
    "            val_loss += hinge_loss(outputs, labels, margin=1.0).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar with current stats\n",
    "            loop2.set_postfix(loss=val_loss/(loop2.n+1), acc=100 * val_correct / val_total)\n",
    "    \n",
    "    # Fixed the print statement (removed the trailing parenthesis)\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader):.4f} Accuracy: {100 * val_correct / val_total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e29039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch.optim as optim\\nfrom tqdm import tqdm\\n\\n# Set device (use GPU if available)\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n# Instantiate model, loss, and optimizer\\nmodel = C3DHybridSVM(num_classes=101).to(device)\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(model.parameters(), lr=0.001)\\n\\n# Training parameters\\nnum_epochs = 50\\n\\n# Ensure data_loader is defined\\nif \\'data_loader1\\' not in locals():\\n    raise NameError(\"data_loader1 is not defined. Please execute the cell defining data_loader before running this cell.\")\\n\\n# Training loop\\nfor epoch in range(num_epochs):\\n    model.train()\\n    running_loss = 0.0\\n    correct = 0\\n    total = 0\\n\\n    loop = tqdm(data_loader1, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\\n\\n    for frames, labels in loop:\\n        frames = frames.to(device)\\n        labels = labels.to(device, dtype=torch.long)  # Now safe to use directly\\n\\n        # Forward pass\\n        outputs = model(frames)\\n        loss = criterion(outputs, labels)\\n\\n        # Backward and optimize\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        # Stats\\n        running_loss += loss.item()\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\n        loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\\n\\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/len(data_loader1):.4f} Accuracy: {100 * correct / total:.2f}%\")'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training loop without val and SVM\n",
    "'''import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate model, loss, and optimizer\n",
    "model = C3DHybridSVM(num_classes=101).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "\n",
    "# Ensure data_loader is defined\n",
    "if 'data_loader1' not in locals():\n",
    "    raise NameError(\"data_loader1 is not defined. Please execute the cell defining data_loader before running this cell.\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(data_loader1, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "    for frames, labels in loop:\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.to(device, dtype=torch.long)  # Now safe to use directly\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(frames)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Stats\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {running_loss/len(data_loader1):.4f} Accuracy: {100 * correct / total:.2f}%\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd87876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as 'output.avi'\n"
     ]
    }
   ],
   "source": [
    "#For Creating an output video with predictions\n",
    "'''import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model = C3D(num_classes=5)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Label mapping\n",
    "index2label = {\n",
    "    0: 'PlayingCello',\n",
    "    1: 'PlayingDhol',\n",
    "    2: 'TennisSwing',\n",
    "    3: 'CricketShot',\n",
    "    4: 'Basketball'\n",
    "}\n",
    "\n",
    "# Video path\n",
    "video_path = r\"data/test/TennisSwing/v_TennisSwing_g05_c03.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Output video writer\n",
    "out = cv2.VideoWriter('output.avi',\n",
    "                      cv2.VideoWriter_fourcc(*'XVID'),\n",
    "                      fps, (width, height))\n",
    "\n",
    "# Parameters\n",
    "clip_len = 16\n",
    "frames_buffer = []\n",
    "label = \"...\"  # Default label before prediction\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    resized = cv2.resize(frame, (112, 112))\n",
    "    rgb_frame = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
    "    frames_buffer.append(rgb_frame)\n",
    "\n",
    "    if len(frames_buffer) == clip_len:\n",
    "        clip = np.array(frames_buffer).astype(np.float32) / 255.0\n",
    "        clip = clip.transpose(3, 0, 1, 2)\n",
    "        input_tensor = torch.tensor(clip).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            label = index2label.get(predicted.item(), \"Unknown\")\n",
    "\n",
    "        frames_buffer.clear()  # Reset for next clip\n",
    "\n",
    "    # Overlay label on frame\n",
    "    cv2.putText(frame, f\"Prediction: {label}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    out.write(frame)  # Write frame with label\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Video saved as 'output.avi'\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74af243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Your model\n",
    "model = C3DHybridSVM(num_classes=5)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Label mapping\n",
    "index2label = {\n",
    "    0: 'PlayingCello',\n",
    "    1: 'PlayingDhol',\n",
    "    2: 'TennisSwing',\n",
    "    3: 'CricketShot',\n",
    "    4: 'Basketball'\n",
    "}\n",
    "\n",
    "# Globals for threading\n",
    "latest_prediction = \"Loading...\"\n",
    "frames_buffer = []\n",
    "clip_len = 16\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Prediction function in background\n",
    "def predict_clip(buffer):\n",
    "    global latest_prediction\n",
    "    clip = np.array(buffer).astype(np.float32) / 255.0\n",
    "    clip = clip.transpose(3, 0, 1, 2)\n",
    "    input_tensor = torch.tensor(clip).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        with lock:\n",
    "            latest_prediction = index2label.get(predicted.item(), \"Unknown\")\n",
    "\n",
    "# Open video\n",
    "video_path = r\"C:\\Users\\Swapn\\Downloads\\Intern\\Projects\\c3d\\data\\test\\Basketball\\v_BasketballDunk_g22_c02.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "delay = 1.0 / fps if fps > 0 else 0.04\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display resized frame\n",
    "    display_frame = frame.copy()\n",
    "    cv2.putText(display_frame, f\"Prediction: {latest_prediction}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Video Classification\", display_frame)\n",
    "\n",
    "    # Prepare input for model (downsampled)\n",
    "    resized = cv2.resize(frame, (112, 112))\n",
    "    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
    "    frames_buffer.append(rgb)\n",
    "\n",
    "    if len(frames_buffer) == clip_len:\n",
    "        # Predict in background\n",
    "        buffer_copy = frames_buffer.copy()\n",
    "        threading.Thread(target=predict_clip, args=(buffer_copy,), daemon=True).start()\n",
    "        frames_buffer.pop(0)  # Slide window\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    time.sleep(delay)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
